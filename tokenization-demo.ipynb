{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29bfab55",
   "metadata": {
    "papermill": {
     "duration": 0.002324,
     "end_time": "2025-02-12T14:18:35.302313",
     "exception": false,
     "start_time": "2025-02-12T14:18:35.299989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of breaking down a text into individual units, called tokens. These tokens can be words, subwords, characters, or even punctuation marks. It's a fundamental step in most Natural Language Processing (NLP) pipelines because it converts raw text into a format that machine learning models can understand. Think of it like separating a sentence into its constituent words before you can analyze its grammar or meaning.\n",
    "\n",
    "Here's why tokenization is important:\n",
    "\n",
    "*   **Machine Learning Compatibility:** Machine learning models work with numerical data. Tokenization converts text into numerical representations (through techniques like word embeddings), making it possible to feed text data into these models.\n",
    "*   **Feature Engineering:** Tokens can be used as features in NLP tasks like text classification, sentiment analysis, and machine translation.\n",
    "*   **Vocabulary Creation:** Tokenization helps in creating a vocabulary of unique tokens, which is essential for many NLP models.\n",
    "\n",
    "Now, let's explore some common tokenization techniques:\n",
    "\n",
    "**1. Word-based Tokenization:**\n",
    "\n",
    "*   **How it works:** This is the simplest form of tokenization. It splits the text into tokens based on spaces or punctuation. Each word is treated as a separate token.\n",
    "*   **Example:** \"The quick brown fox.\" becomes `[\"The\", \"quick\", \"brown\", \"fox\", \".\"]`\n",
    "*   **Advantages:** Easy to implement.\n",
    "*   **Disadvantages:**\n",
    "    *   Doesn't handle out-of-vocabulary (OOV) words well. If a word isn't in the model's vocabulary, it's often represented as `<UNK>`, losing information.\n",
    "    *   Can lead to a very large vocabulary, especially for languages with many words or inflections. This can be computationally expensive.\n",
    "    *   Doesn't capture the meaning of subwords (e.g., \"unbreakable\" has meaning in \"un,\" \"break,\" and \"able\").\n",
    "\n",
    "**2. Character-based Tokenization:**\n",
    "\n",
    "*   **How it works:** Treats each character as a token.\n",
    "*   **Example:** \"Hello\" becomes `[\"H\", \"e\", \"l\", \"l\", \"o\"]`\n",
    "*   **Advantages:**\n",
    "    *   Handles OOV words well. Any word can be represented as a sequence of characters.\n",
    "    *   Small vocabulary size.\n",
    "*   **Disadvantages:**\n",
    "    *   Doesn't capture the meaning of words or subwords directly.\n",
    "    *   Sequences can become very long, requiring more computational resources.\n",
    "\n",
    "**3. Subword Tokenization:**\n",
    "\n",
    "Subword tokenization is a middle ground between word-based and character-based tokenization. It breaks words into smaller units (subwords) that can be morphemes (meaningful units like \"un,\" \"break,\" \"able\") or frequently occurring character sequences. This approach addresses the OOV problem and manages vocabulary size more effectively. Here are a few popular subword tokenization methods:\n",
    "\n",
    "*   **a) WordPiece:**\n",
    "    *   **How it works:** Starts with a small vocabulary of characters and iteratively merges the most frequent character pairs to form subwords. It continues until a desired vocabulary size is reached. It's often used in models like BERT.\n",
    "    *   **Example:** Imagine starting with `[\"b\", \"a\", \"t\", \"s\", \"c\", \"a\", \"t\", \"s\"]` and iteratively merging the most frequent pairs. You might get \"bat,\" \"cats,\" and then \"bats\" (if they are frequent enough).\n",
    "    *   **Advantages:** Balances vocabulary size and handles OOV words.\n",
    "\n",
    "*   **b) Byte-Pair Encoding (BPE):**\n",
    "    *   **How it works:** Similar to WordPiece, BPE also starts with a small vocabulary and iteratively merges the most frequent byte pairs (or character pairs) to form new tokens. It's widely used in various language models.\n",
    "    *   **Example:** The process is conceptually the same as WordPiece.\n",
    "    *   **Advantages:** Efficient and commonly used.\n",
    "\n",
    "*   **c) SentencePiece:**\n",
    "    *   **How it works:** SentencePiece is a bit different. It treats spaces as a regular character and builds the vocabulary based on the most frequent subword units. It can be used for both subword and character-level tokenization. It's often used in models like XLNet and ALBERT.\n",
    "    *   **Example:** Instead of splitting \"New York,\" it might learn \" New\" and \" York\" as separate tokens (note the space).\n",
    "    *   **Advantages:** Handles whitespace consistently and can be used for languages without clear word boundaries. It also makes detokenization (converting tokens back to text) straightforward.\n",
    "\n",
    "**Key Differences and Summary:**\n",
    "\n",
    "| Technique        | Unit of Tokenization | Handles OOV | Vocabulary Size |\n",
    "|-----------------|----------------------|-------------|-----------------|\n",
    "| Word-based      | Word                 | Poor        | Large           |\n",
    "| Character-based | Character            | Excellent   | Small           |\n",
    "| WordPiece       | Subword              | Good        | Moderate        |\n",
    "| BPE             | Subword              | Good        | Moderate        |\n",
    "| SentencePiece   | Subword/Character    | Good        | Moderate        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69156ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T14:18:35.307633Z",
     "iopub.status.busy": "2025-02-12T14:18:35.307226Z",
     "iopub.status.idle": "2025-02-12T14:18:35.312527Z",
     "shell.execute_reply": "2025-02-12T14:18:35.311660Z"
    },
    "papermill": {
     "duration": 0.009697,
     "end_time": "2025-02-12T14:18:35.314202",
     "exception": false,
     "start_time": "2025-02-12T14:18:35.304505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install tokenizers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973559ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T14:18:35.319349Z",
     "iopub.status.busy": "2025-02-12T14:18:35.319006Z",
     "iopub.status.idle": "2025-02-12T14:18:35.487519Z",
     "shell.execute_reply": "2025-02-12T14:18:35.486326Z"
    },
    "papermill": {
     "duration": 0.172991,
     "end_time": "2025-02-12T14:18:35.489275",
     "exception": false,
     "start_time": "2025-02-12T14:18:35.316284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SentencePiece Tokenization:\n",
      "['▁', 'T', 'h', 'i', 's', '▁i', 's', '▁a', '▁sample', '▁text', '.', '▁', 'I', 't', '▁', 'in', 'c', 'l', 'u', 'de', 's', '▁s', 'o', 'm', 'e', '▁', 'word', 's', '▁th', 'at', '▁', 'ar', 'e', '▁re', 'p', 'e', 'ate', 'd', ',', '▁', 'li', 'ke', '▁sample', '.', '▁', 'W', 'e', '▁w', 'i', 'l', 'l', '▁', 'u', 's', 'e', '▁th', 'i', 's', '▁text', '▁to', '▁', 'de', 'm', 'on', 's', 't', 'r', 'ate', '▁s', 'u', 'b', 'word', '▁tokenization', '.', '▁', 'S', 'u', 'b', 'word', '▁tokenization', '▁i', 's', '▁i', 'mp', 'or', 't', 'an', 't', '▁', 'f', 'or', '▁', 'h', 'and', 'l', 'in', 'g', '▁', 'o', 'u', 't', '-', 'o', 'f', '-', 'v', 'o', 'c', 'a', 'b', 'u', 'l', 'ar', 'y', '▁', 'word', 's', '▁', 'and', '▁re', 'd', 'u', 'c', 'in', 'g', '▁', 'v', 'o', 'c', 'a', 'b', 'u', 'l', 'ar', 'y', '▁s', 'iz', 'e', '.']\n",
      "\n",
      "Detokenized SentencePiece:\n",
      "This is a sample text. It includes some words that are repeated, like sample. We will use this text to demonstrate subword tokenization. Subword tokenization is important for handling out-of-vocabulary words and reducing vocabulary size.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Sample text (you would use your own data here)\n",
    "text = \"\"\"This is a sample text.  It includes some words that are repeated, like sample.\n",
    "We will use this text to demonstrate subword tokenization.  Subword tokenization is important\n",
    "for handling out-of-vocabulary words and reducing vocabulary size.\n",
    "\"\"\"\n",
    "\n",
    "# The libraries require training from a file, so we'll save our text to a file\n",
    "with open(\"text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "# SentencePiece Tokenization (using the 'sentencepiece' library)\n",
    "\n",
    "# Train SentencePiece\n",
    "spm.SentencePieceTrainer.Train(\n",
    "                               input='text.txt',\n",
    "                               model_prefix='sentencepiece_model',  # Output model file prefix\n",
    "                               vocab_size=60,\n",
    "                               character_coverage=0.9995,  # Adjust as needed\n",
    "                               train_extremely_large_corpus=True) # Important for large text files, otherwise it may crash.\n",
    "\n",
    "# Load the SentencePiece model\n",
    "sentencepiece_tokenizer = spm.SentencePieceProcessor()\n",
    "sentencepiece_tokenizer.Load(\"sentencepiece_model.model\")\n",
    "\n",
    "# Tokenize with SentencePiece\n",
    "sentencepiece_output = sentencepiece_tokenizer.EncodeAsPieces(text) # EncodeAsIds to get numerical ids.\n",
    "\n",
    "print(\"\\nSentencePiece Tokenization:\")\n",
    "print(sentencepiece_output)\n",
    "\n",
    "# Example of detokenization (converting tokens back to text)\n",
    "\n",
    "detokenized_sentencepiece = sentencepiece_tokenizer.DecodePieces(sentencepiece_output)\n",
    "print(\"\\nDetokenized SentencePiece:\")\n",
    "print(detokenized_sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9f659",
   "metadata": {
    "papermill": {
     "duration": 0.001664,
     "end_time": "2025-02-12T14:18:35.493055",
     "exception": false,
     "start_time": "2025-02-12T14:18:35.491391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Further Reads\n",
    "\n",
    "https://huggingface.co/learn/nlp-course/en/chapter6/6\n",
    "\n",
    "https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb#scrollTo=a0ima0_TfmSA"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.579613,
   "end_time": "2025-02-12T14:18:36.014677",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-12T14:18:32.435064",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
