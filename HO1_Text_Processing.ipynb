{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "HO1: Resume Text - Processing and Tokenization",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranalibose/LangVisionWorkshop/blob/main/HO1_Text_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Preprocessing Steps**\n",
        "\n",
        "Before feeding text data into a machine learning model, it must be cleaned and standardized. Below are some key **text preprocessing steps**:\n",
        "\n",
        "### **1. Stopwords Removal**\n",
        "- Stopwords are common words (e.g., \"the\", \"is\", \"and\") that do not contribute much meaning to text analysis.\n",
        "- Removing stopwords helps reduce dimensionality and improves computational efficiency.\n",
        "- Example using NLTK in Python:\n",
        "  ```python\n",
        "  from nltk.corpus import stopwords\n",
        "  from nltk.tokenize import word_tokenize\n",
        "  import nltk\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('punkt')\n",
        "  \n",
        "  text = \"This is an example of text preprocessing in NLP.\"\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  words = word_tokenize(text)\n",
        "  filtered_text = [word for word in words if word.lower() not in stop_words]\n",
        "  print(filtered_text)  # ['example', 'text', 'preprocessing', 'NLP', '.']\n",
        "  ```\n",
        "\n",
        "### **2. Tokenization**\n",
        "- Tokenization splits text into meaningful units called **tokens**.\n",
        "- Types of tokenization:\n",
        "  - **Word-based**: Splits at spaces (e.g., \"Machine Learning\" â†’ [\"Machine\", \"Learning\"]).\n",
        "  - **Subword-based**: Uses WordPiece, Byte-Pair Encoding (BPE), or SentencePiece.\n",
        "  - **Character-based**: Splits into individual characters (useful for languages without spaces).\n",
        "\n",
        "- Example using NLTK:\n",
        "  ```python\n",
        "  words = word_tokenize(\"Tokenization splits text into words.\")\n",
        "  print(words)  # ['Tokenization', 'splits', 'text', 'into', 'words', '.']\n",
        "  ```\n",
        "\n",
        "\n",
        "## **What are Embeddings?**\n",
        "- Embeddings are **numerical representations of words or documents** in a vector space.\n",
        "- They capture **semantic relationships** between words.\n",
        "- Example: Word vectors for **\"king\"**, **\"queen\"**, and **\"man\"** may capture the analogy:\n",
        "  \n",
        "  \\[ \\text{King} - \\text{Man} + \\text{Woman} \\approx \\text{Queen} \\]\n",
        "\n",
        "### **Types of Embeddings**\n",
        "\n",
        "### **1. TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "- Measures the importance of words in a document relative to a collection of documents.\n",
        "- Formula:\n",
        "  \\[\n",
        "  TF(w) = \\frac{\\text{Number of times word w appears in document}}{\\text{Total words in document}}\n",
        "  \\]\n",
        "  \\[\n",
        "  IDF(w) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing w}}\\right)\n",
        "  \\]\n",
        "  \\[\n",
        "  \\text{TF-IDF}(w) = \\text{TF}(w) \\times \\text{IDF}(w)\n",
        "  \\]\n",
        "\n",
        "- Example using Scikit-Learn:\n",
        "  ```python\n",
        "  from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  corpus = [\"This is a sample document.\", \"This document is about NLP.\"]\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "  print(tfidf_matrix.toarray())\n",
        "  ```\n",
        "\n",
        "### **2. Word2Vec**\n",
        "- Uses a **neural network** to learn word relationships.\n",
        "- Two approaches:\n",
        "  - **CBOW (Continuous Bag of Words)**: Predicts a target word from surrounding words.\n",
        "    \\[\n",
        "    P(w_t | w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2})\n",
        "    \\]\n",
        "  - **Skip-Gram**: Predicts surrounding words given a target word.\n",
        "    \\[\n",
        "    P(w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2} | w_t)\n",
        "    \\]\n",
        "\n",
        "- Example using Gensim:\n",
        "  ```python\n",
        "  from gensim.models import Word2Vec\n",
        "  sentences = [[\"machine\", \"learning\", \"is\", \"fun\"], [\"deep\", \"learning\", \"is\", \"awesome\"]]\n",
        "  model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "  print(model.wv[\"learning\"])  # Word vector for 'learning'\n",
        "  ```\n",
        "\n",
        "\n",
        "## **Summary of Embeddings**\n",
        "| Embedding Type | How It Works | Strengths | Weaknesses |\n",
        "|---------------|-------------|-----------|------------|\n",
        "| **TF-IDF** | Uses frequency and importance in documents | Simple and effective for keyword-based tasks | Ignores word meaning and order |\n",
        "| **Word2Vec** | Learns vector representations using neural networks | Captures semantic relationships | Requires a large corpus for good results |\n",
        "\n",
        "Would you like an example of embeddings applied to a real-world NLP problem? ðŸš€\n"
      ],
      "metadata": {
        "id": "0aXSfm_umgym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nltk spacy numpy gensim scikit-learn"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T08:55:34.469412Z",
          "iopub.execute_input": "2025-02-13T08:55:34.470241Z",
          "iopub.status.idle": "2025-02-13T08:55:34.474721Z",
          "shell.execute_reply.started": "2025-02-13T08:55:34.470194Z",
          "shell.execute_reply": "2025-02-13T08:55:34.473614Z"
        },
        "id": "cp60eBHhmgyo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import spacy\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load SpaCy NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Sample Resume Text (Replace this with real input)\n",
        "resume_text = \"\"\"\n",
        "John Doe\n",
        "Software Engineer\n",
        "\n",
        "Education: B.Sc. in Computer Science, XYZ University\n",
        "Experience: 3 years in software development\n",
        "Skills: Python, Machine Learning, NLP, TensorFlow\n",
        "\"\"\"\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize words\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Function to extract sections\n",
        "def extract_sections(text):\n",
        "    sections = {}\n",
        "    patterns = {\n",
        "        \"education\": r\"education[:\\-\\n](.*?)(?=experience|skills|$)\",\n",
        "        \"experience\": r\"experience[:\\-\\n](.*?)(?=education|skills|$)\",\n",
        "        \"skills\": r\"skills[:\\-\\n](.*?)(?=education|experience|$)\"\n",
        "    }\n",
        "\n",
        "    for key, pattern in patterns.items():\n",
        "        match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "        if match:\n",
        "            sections[key] = preprocess_text(match.group(1))\n",
        "        else:\n",
        "            sections[key] = \"\"\n",
        "    return sections\n",
        "\n",
        "# Preprocess resume text\n",
        "clean_text = preprocess_text(resume_text)\n",
        "sections = extract_sections(resume_text)\n",
        "\n",
        "print(\"Cleaned Resume Text:\", clean_text)\n",
        "print(\"Extracted Sections:\", sections)\n",
        "\n",
        "# Tokenization and Embeddings\n",
        "\n",
        "## TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([clean_text])\n",
        "print(\"TF-IDF Embedding Shape:\", tfidf_matrix.shape)\n",
        "\n",
        "## Word2Vec Embeddings\n",
        "def generate_word2vec_embeddings(text):\n",
        "    sentences = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
        "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    word_vectors = {word: model.wv[word] for word in model.wv.index_to_key}\n",
        "    return model, word_vectors\n",
        "\n",
        "word2vec_model, word_vectors = generate_word2vec_embeddings(clean_text)\n",
        "print(\"Word2Vec Vocabulary Size:\", len(word_vectors))\n",
        "\n",
        "# Example usage of embeddings\n",
        "example_word = \"python\"\n",
        "if example_word in word_vectors:\n",
        "    print(f\"Embedding for '{example_word}':\", word_vectors[example_word])\n",
        "else:\n",
        "    print(f\"'{example_word}' not found in vocabulary\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T08:54:53.890005Z",
          "iopub.execute_input": "2025-02-13T08:54:53.890405Z",
          "iopub.status.idle": "2025-02-13T08:55:34.467997Z",
          "shell.execute_reply.started": "2025-02-13T08:54:53.890363Z",
          "shell.execute_reply": "2025-02-13T08:55:34.466693Z"
        },
        "id": "ZOfIqcQlmgyq",
        "outputId": "e644b3f8-3724-4cc7-8f9f-90e26bbcd21e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nCleaned Resume Text: john doe software engineer education bsc computer science xyz university experience 3 years software development skills python machine learning nlp tensorflow\nExtracted Sections: {'education': 'bsc computer science xyz university', 'experience': '3 years software development', 'skills': 'python machine learning nlp tensorflow'}\nTF-IDF Embedding Shape: (1, 19)\nWord2Vec Vocabulary Size: 20\nEmbedding for 'python': [ 1.2923904e-03 -9.7982762e-03  4.5899418e-03 -5.3133053e-04\n  6.3345684e-03  1.7845632e-03 -3.1254247e-03  7.7570681e-03\n  1.5504272e-03  5.8964350e-05 -4.6103387e-03 -8.4572500e-03\n -7.7618901e-03  8.6681144e-03 -8.9204954e-03  9.0352260e-03\n -9.2851855e-03 -2.7858073e-04 -1.9112782e-03 -8.9328075e-03\n  8.6294059e-03  6.7847371e-03  3.0244568e-03  4.8295590e-03\n  1.1414578e-04  9.4253253e-03  7.0194174e-03 -9.8572774e-03\n -4.4332640e-03 -1.2960931e-03  3.0509743e-03 -4.3275156e-03\n  1.4534771e-03 -7.8378087e-03  2.7765711e-03  4.7062528e-03\n  4.9372939e-03 -3.1720817e-03 -8.4256670e-03 -9.2233820e-03\n -7.2323863e-04 -7.3244264e-03 -6.8165022e-03  6.1186352e-03\n  7.1719605e-03  2.1173442e-03 -7.8990990e-03 -5.6964462e-03\n  8.0486499e-03  3.9181150e-03 -5.2402392e-03 -7.3907939e-03\n  7.7237556e-04  3.4625749e-03  2.0810477e-03  3.0984627e-03\n -5.6184717e-03 -9.8893233e-03 -7.0164008e-03  2.2666225e-04\n  4.6145036e-03  4.5233420e-03  1.8851999e-03  5.1684850e-03\n -1.1039107e-04  4.1117677e-03 -9.1264863e-03  7.7036023e-03\n  6.1419350e-03  5.1259720e-03  7.1997158e-03  8.4410151e-03\n  7.4181124e-04 -1.7010336e-03  5.1412347e-04 -9.3195830e-03\n  8.4017627e-03 -6.3856319e-03  8.4227799e-03 -4.2456496e-03\n  6.4648484e-04 -9.1686863e-03 -9.5545091e-03 -7.8351470e-03\n -7.7288668e-03  3.7645915e-04 -7.2273957e-03 -4.9551986e-03\n -5.2715307e-03 -4.2844894e-03  7.0070573e-03  4.8310612e-03\n  8.6834570e-03  7.0913867e-03 -5.6882380e-03  7.2452449e-03\n -9.2931828e-03 -2.5816548e-03 -7.7522970e-03  4.1900817e-03]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_resume_sections(resume_text):\n",
        "    \"\"\"\n",
        "    Extracts Education, Experience, and Skills sections from resume text.\n",
        "    \"\"\"\n",
        "\n",
        "    sections = {}\n",
        "\n",
        "    # Regular expressions to identify section headers (customize as needed)\n",
        "    section_headers = {\n",
        "        \"education\": r\"\\bEducation\\b|\\bQualifications\\b|\\bAcademic Details\\b\",  # Add variations\n",
        "        \"experience\": r\"\\bExperience\\b|\\bProfessional Experience\\b|\\bWork History\\b\",  # Add variations\n",
        "        \"skills\": r\"\\bSkills\\b|\\bTechnical Skills\\b|\\bCore Competencies\\b|\\bAreas of Expertise\\b\",  # Add variations\n",
        "        \"summary\": r\"\\bSummary\\b|\\bObjective\\b|\\bCareer Objective\\b|\\bProfessional Summary\\b\" #Add variations\n",
        "    }\n",
        "\n",
        "    start_indices = {}\n",
        "    end_indices = {}\n",
        "\n",
        "    for section_name, pattern in section_headers.items():\n",
        "        match = re.search(pattern, resume_text, re.IGNORECASE | re.MULTILINE)\n",
        "        if match:\n",
        "            start_indices[section_name] = match.start()\n",
        "        else:\n",
        "          print(f\"Section '{section_name}' not found.\")\n",
        "\n",
        "\n",
        "    # Find end indices (start of next section or end of text)\n",
        "    for section_name in section_headers:\n",
        "       if section_name in start_indices:\n",
        "          next_section_index = len(resume_text) # Default to end of text\n",
        "          for other_section in section_headers:\n",
        "              if other_section != section_name and other_section in start_indices and start_indices[other_section] > start_indices[section_name]:\n",
        "                  next_section_index = min(next_section_index, start_indices[other_section])\n",
        "          end_indices[section_name] = next_section_index\n",
        "          sections[section_name] = resume_text[start_indices[section_name]:end_indices[section_name]].strip()\n",
        "\n",
        "\n",
        "    return sections\n",
        "\n",
        "resume_text = \"\"\"\n",
        "Summary\n",
        "A highly motivated and results-oriented professional looking to leverage my strong Machine Learning Skills.\n",
        "\n",
        "Education\n",
        "Bachelor of Science in Computer Science, University X, 2020-2024\n",
        "Master of Technology in Data Science, University Y, 2024-2026\n",
        "\n",
        "Experience\n",
        "Software Engineer Intern, Company A, May 2023 - Aug 2023\n",
        "Data Scientist, Company B, Jan 2024 - Present\n",
        "\n",
        "Skills\n",
        "Python, Java, Machine Learning, Deep Learning, NLP, Communication, Teamwork\n",
        "\"\"\"\n",
        "\n",
        "extracted_sections = extract_resume_sections(resume_text)\n",
        "\n",
        "for section_name, content in extracted_sections.items():\n",
        "    print(f\"\\n{section_name.upper()}\")\n",
        "    print(content)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T09:04:51.139744Z",
          "iopub.execute_input": "2025-02-13T09:04:51.140205Z",
          "iopub.status.idle": "2025-02-13T09:04:51.151557Z",
          "shell.execute_reply.started": "2025-02-13T09:04:51.140169Z",
          "shell.execute_reply": "2025-02-13T09:04:51.150126Z"
        },
        "id": "OUvA40NXmgyr",
        "outputId": "a96aafb6-7f9c-415d-d027-9e19f42a7da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nEDUCATION\nEducation\nBachelor of Science in Computer Science, University X, 2020-2024\nMaster of Technology in Data Science, University Y, 2024-2026\n\nEXPERIENCE\nExperience\nSoftware Engineer Intern, Company A, May 2023 - Aug 2023\nData Scientist, Company B, Jan 2024 - Present\n\nSkills\nPython, Java, Machine Learning, Deep Learning, NLP, Communication, Teamwork\n\nSKILLS\nSkills.\n\nSUMMARY\nSummary\nA highly motivated and results-oriented professional looking to leverage my strong Machine Learning\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}