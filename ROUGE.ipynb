{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcdJI7eJK0/MDX15yNdmku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranalibose/LangVisionWorkshop/blob/main/ROUGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROUGE Scores: An Overview\n",
        "\n",
        "## What is ROUGE?\n",
        "ROUGE (**Recall-Oriented Understudy for Gisting Evaluation**) is a set of metrics used to evaluate **text summarization and text generation tasks** by comparing machine-generated text to human-written references.\n",
        "\n",
        "## Types of ROUGE Scores\n",
        "\n",
        "### 1. ROUGE-N (N-Gram Overlap)\n",
        "- Measures **n-gram** (sequence of N words) overlap between generated and reference text.\n",
        "- **Formula:**  \n",
        "  \\[\n",
        "  ROUGE-N = \\frac{\\text{Number of overlapping n-grams}}{\\text{Total n-grams in reference}}\n",
        "  \\]\n",
        "- Common variations:\n",
        "  - **ROUGE-1** (unigrams) â€“ measures **word-level** overlap.\n",
        "  - **ROUGE-2** (bigrams) â€“ considers **phrase-level** overlap.\n",
        "  - **ROUGE-L** (longest common subsequence) â€“ captures **sentence fluency and structure**.\n",
        "\n",
        "### 2. ROUGE-L (Longest Common Subsequence - LCS)\n",
        "- Measures the **longest matching sequence of words** between generated and reference text.\n",
        "- Captures sentence **fluency and coherence**, unlike n-grams which may not preserve order.\n",
        "\n",
        "### 3. ROUGE-W (Weighted LCS)\n",
        "- Similar to ROUGE-L but **assigns more weight** to longer consecutive matches, emphasizing readability.\n",
        "\n",
        "### 4. ROUGE-S (Skip-Bigram-Based Co-Occurrence)\n",
        "- Measures overlap of **pairs of words** allowing gaps in between (non-consecutive matches).\n",
        "\n",
        "## What is a Good ROUGE Score?\n",
        "- **ROUGE-1 (~40-60%)** is considered **good** for abstractive summarization.\n",
        "- **ROUGE-2 (~15-30%)** is expected, as bigram matches are less frequent.\n",
        "- **ROUGE-L (~40-50%)** is ideal for fluency and coherence.\n",
        "- Higher **recall** is often preferred in summarization tasks since the generated text should **cover key information** from the reference.\n",
        "\n",
        "ðŸ”¹ **Example Interpretation**:  \n",
        "A **ROUGE-1 score of 50%** means **half of the words in the reference summary appear in the generated summary.**\n",
        "\n"
      ],
      "metadata": {
        "id": "oFAV4aZ-5Kpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install rouge"
      ],
      "metadata": {
        "id": "sgXo3DsV5ofw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "def calculate_rouge(reference, generated):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(generated, reference)\n",
        "    return scores\n",
        "\n",
        "# Example reference and generated summary\n",
        "reference_summary = \"The cat sat on the mat and looked outside the window.\"\n",
        "generated_summary = \"The cat was sitting on the mat and gazing through the window.\"\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_scores = calculate_rouge(reference_summary, generated_summary)\n",
        "print(\"ROUGE Scores:\", rouge_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cLfO2NK5hlX",
        "outputId": "719ee207-c3c1-4405-fad4-f63c1609d9c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores: [{'rouge-1': {'r': 0.7, 'p': 0.6363636363636364, 'f': 0.6666666616780046}, 'rouge-2': {'r': 0.5, 'p': 0.45454545454545453, 'f': 0.47619047120181407}, 'rouge-l': {'r': 0.7, 'p': 0.6363636363636364, 'f': 0.6666666616780046}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "opzdym4n5idO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}